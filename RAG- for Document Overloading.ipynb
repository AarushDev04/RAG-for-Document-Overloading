{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDHDMAyP-EqH",
        "outputId": "e86373e8-5d27-400e-89a7-96a6fd626b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.27)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pncnF2pYFQAt",
        "outputId": "d901fb6f-ed9c-4df4-9d46-d30e5be53633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.27)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qtpTOI4zEzt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.llms import Ollama\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "from langchain.callbacks.manager import CallbackManagerForRetrieverRun"
      ],
      "metadata": {
        "id": "pgfTYBNYE7U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core dependencies\n",
        "!pip install -U sentence-transformers\n",
        "!pip install transformers torch\n",
        "!pip install numpy\n",
        "\n",
        "# Install MCP (Model Context Protocol)\n",
        "!pip install mcp\n",
        "\n",
        "# Install Ollama Python client\n",
        "!pip install ollama\n",
        "\n",
        "# Additional dependencies\n",
        "!pip install asyncio-mqtt\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvUsF34vE9sQ",
        "outputId": "3ae1f9ed-e166-4afd-d47f-86f69327737b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: mcp in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.10.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.1)\n",
            "Requirement already satisfied: httpx>=0.27.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.28.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp) (2.10.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.11.0 in /usr/local/lib/python3.12/dist-packages (from mcp) (2.11.7)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.47.3)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.35.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (4.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.1->mcp) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.0->mcp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.0->mcp) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.11.0->mcp) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp) (1.1.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp) (8.2.1)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.5.4)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.11.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Requirement already satisfied: asyncio-mqtt in /usr/local/lib/python3.12/dist-packages (0.16.2)\n",
            "Requirement already satisfied: paho-mqtt>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from asyncio-mqtt) (2.1.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb langchain sentence-transformers ollama mcp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4StuNvVqFfyI",
        "outputId": "9395857c-8067-4e59-cfa2-fe2512b06177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.5.4)\n",
            "Requirement already satisfied: mcp in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.10.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.47.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (1.3.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama (Linux/Mac)\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "\n",
        "# Or for Windows, download from: https://ollama.ai/download\n",
        "\n",
        "# Pull LLAMA3 model\n",
        "!ollama pull llama3\n",
        "\n",
        "# Verify installation\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVNiV61SHONr",
        "outputId": "111d09cf-0549-4cb3-fbc1-e15738ba76f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Error: ollama server not responding - could not connect to ollama server, run 'ollama serve' to start it\n",
            "Error: ollama server not responding - could not connect to ollama server, run 'ollama serve' to start it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import uuid\n"
      ],
      "metadata": {
        "id": "9vtL4-VhfDRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain.embeddings import HuggingFaceEmbeddings\n",
        "    from langchain.schema import Document\n",
        "    from langchain.llms import Ollama\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from langchain.vectorstores.base import VectorStore\n",
        "    from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
        "except ImportError as e:\n",
        "    print(f\"LangChain import error: {e}\")\n",
        "    print(\"Install with: pip install langchain langchain-community\")\n"
      ],
      "metadata": {
        "id": "BP55Zbf8fPI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import chromadb\n",
        "    from chromadb.config import Settings\n",
        "    from chromadb.utils import embedding_functions\n",
        "except ImportError as e:\n",
        "    print(f\"ChromaDB import error: {e}\")\n",
        "    print(\"Install with: pip install chromadb\")\n"
      ],
      "metadata": {
        "id": "hr716T4yfTnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from mcp.server import Server\n",
        "    from mcp.types import Resource, Tool\n",
        "    import mcp.server.stdio\n",
        "except ImportError as e:\n",
        "    print(f\"MCP import error: {e}\")\n",
        "    print(\"Install with: pip install mcp\")"
      ],
      "metadata": {
        "id": "OT625GlhfW2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "AjA5pnl0fZ7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ClassifiedDocument:\n",
        "    \"\"\"Represents a document\"\"\"\n",
        "    content: str\n",
        "    metadata: Dict[str, Any]\n",
        "    source: str"
      ],
      "metadata": {
        "id": "4JCkOSPZfdYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class EmbeddedChunk:\n",
        "    \"\"\"Represents a document chunk with embedding\"\"\"\n",
        "    id: str\n",
        "    content: str\n",
        "    embedding: List[float]\n",
        "    metadata: Dict[str, Any]\n",
        "    parent_doc_id: str"
      ],
      "metadata": {
        "id": "wnfNS5jifg6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"Handles document parsing and chunking\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def parse_document(self, file_path: str) -> ClassifiedDocument:\n",
        "        \"\"\"Parse document from file path\"\"\"\n",
        "        path = Path(file_path)\n",
        "\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "        # Handle different file types\n",
        "        content = \"\"\n",
        "        if path.suffix.lower() == '.txt':\n",
        "            content = path.read_text(encoding='utf-8')\n",
        "        elif path.suffix.lower() == '.json':\n",
        "            data = json.loads(path.read_text(encoding='utf-8'))\n",
        "            content = json.dumps(data, indent=2)\n",
        "        elif path.suffix.lower() in ['.md', '.markdown']:\n",
        "            content = path.read_text(encoding='utf-8')\n",
        "        elif path.suffix.lower() == '.csv':\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(path)\n",
        "            content = df.to_string()\n",
        "        else:\n",
        "            # For other formats, try to read as text\n",
        "            try:\n",
        "                content = path.read_text(encoding='utf-8')\n",
        "            except UnicodeDecodeError:\n",
        "                content = path.read_text(encoding='latin-1')\n",
        "\n",
        "        metadata = {\n",
        "            \"file_name\": path.name,\n",
        "            \"file_path\": str(path),\n",
        "            \"file_size\": path.stat().st_size,\n",
        "            \"file_type\": path.suffix.lower()\n",
        "        }\n",
        "\n",
        "        return ClassifiedDocument(\n",
        "            content=content,\n",
        "            metadata=metadata,\n",
        "            source=file_path\n",
        "        )\n",
        "\n",
        "    def chunk_document(self, doc: ClassifiedDocument) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Split document into chunks\"\"\"\n",
        "        chunks = self.text_splitter.split_text(doc.content)\n",
        "\n",
        "        chunked_docs = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_id = hashlib.md5(f\"{doc.source}_{i}_{chunk[:100]}\".encode()).hexdigest()\n",
        "\n",
        "            chunked_docs.append({\n",
        "                \"id\": chunk_id,\n",
        "                \"content\": chunk,\n",
        "                \"chunk_index\": i,\n",
        "                \"parent_doc_id\": hashlib.md5(doc.source.encode()).hexdigest(),\n",
        "                \"metadata\": {\n",
        "                    **doc.metadata,\n",
        "                    \"chunk_size\": len(chunk),\n",
        "                    \"total_chunks\": len(chunks)\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return chunked_docs"
      ],
      "metadata": {
        "id": "cnHF5_CCflIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClasswiseEmbedder:\n",
        "    \"\"\"Handles embedding generation\"\"\" # Updated description\n",
        "\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': 'cpu'},\n",
        "            encode_kwargs={'normalize_embeddings': True}\n",
        "        )\n",
        "        # self.class_embedders = {}  # Removed class-wise cache\n",
        "\n",
        "    def embed_chunks(self, chunks: List[Dict[str, Any]]) -> List[EmbeddedChunk]:\n",
        "        \"\"\"Generate embeddings for chunks\"\"\" # Updated description\n",
        "        embedded_chunks = []\n",
        "\n",
        "        # Extract content for batch embedding\n",
        "        texts = [chunk[\"content\"] for chunk in chunks]\n",
        "\n",
        "        # Generate embeddings in batch\n",
        "        embeddings = self.embeddings.embed_documents(texts)\n",
        "\n",
        "        # Create embedded chunks\n",
        "        for chunk, embedding in zip(chunks, embeddings):\n",
        "            embedded_chunk = EmbeddedChunk(\n",
        "                id=chunk[\"id\"],\n",
        "                content=chunk[\"content\"],\n",
        "                embedding=embedding,\n",
        "                # classification=classification, # Removed classification\n",
        "                metadata=chunk[\"metadata\"],\n",
        "                parent_doc_id=chunk[\"parent_doc_id\"]\n",
        "            )\n",
        "            embedded_chunks.append(embedded_chunk)\n",
        "\n",
        "        return embedded_chunks"
      ],
      "metadata": {
        "id": "GDEl0lciftVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChromaVectorStore:\n",
        "    \"\"\"ChromaDB-based vector store\"\"\" # Updated description\n",
        "\n",
        "    def __init__(self, persist_directory: str = \"./chroma_db\", collection_name: str = \"rag_documents\"):\n",
        "        self.persist_directory = persist_directory\n",
        "        self.collection_name = collection_name\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "        self.embedding_function = None # This might be needed later if not using LangChain's embedder\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
        "        # Create ChromaDB client with persistence\n",
        "        settings = Settings(\n",
        "            persist_directory=self.persist_directory,\n",
        "            anonymized_telemetry=False\n",
        "        )\n",
        "\n",
        "        self.client = chromadb.PersistentClient(\n",
        "            path=self.persist_directory,\n",
        "            settings=settings\n",
        "        )\n",
        "\n",
        "        # Create or get collection\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=self.collection_name)\n",
        "            logger.info(f\"Using existing collection: {self.collection_name}\")\n",
        "        except:\n",
        "            self.collection = self.client.create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"hnsw:space\": \"cosine\"}\n",
        "            )\n",
        "            logger.info(f\"Created new collection: {self.collection_name}\")\n",
        "\n",
        "        logger.info(\"ChromaDB initialized successfully\")\n",
        "\n",
        "    def add_embeddings(self, embedded_chunks: List[EmbeddedChunk]):\n",
        "        \"\"\"Store embedded chunks in ChromaDB\"\"\"\n",
        "        if not self.collection:\n",
        "            raise ValueError(\"ChromaDB not initialized. Call initialize() first.\")\n",
        "\n",
        "        # Prepare data for ChromaDB\n",
        "        ids = []\n",
        "        embeddings = []\n",
        "        documents = []\n",
        "        metadatas = []\n",
        "\n",
        "        for chunk in embedded_chunks:\n",
        "            ids.append(chunk.id)\n",
        "            embeddings.append(chunk.embedding)\n",
        "            documents.append(chunk.content)\n",
        "\n",
        "            # Combine all metadata\n",
        "            metadata = {\n",
        "                \"parent_doc_id\": chunk.parent_doc_id,\n",
        "                **chunk.metadata\n",
        "            }\n",
        "            metadatas.append(metadata)\n",
        "\n",
        "        # Add to collection in batches (ChromaDB has batch size limits)\n",
        "        batch_size = 1000\n",
        "        for i in range(0, len(ids), batch_size):\n",
        "            batch_ids = ids[i:i+batch_size]\n",
        "            batch_embeddings = embeddings[i:i+batch_size]\n",
        "            batch_documents = documents[i:i+batch_size]\n",
        "            batch_metadatas = metadatas[i:i+batch_size]\n",
        "\n",
        "            self.collection.add(\n",
        "                ids=batch_ids,\n",
        "                embeddings=batch_embeddings,\n",
        "                documents=batch_documents,\n",
        "                metadatas=batch_metadatas\n",
        "            )\n",
        "\n",
        "        logger.info(f\"Stored {len(embedded_chunks)} embedded chunks in ChromaDB\")\n",
        "\n",
        "    def similarity_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Perform similarity search\"\"\" # Updated description\n",
        "        if not self.collection:\n",
        "            raise ValueError(\"ChromaDB not initialized. Call initialize() first.\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        query_embedding = embedder.embed_query(query)\n",
        "\n",
        "        # Query ChromaDB\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=k,\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "        )\n",
        "\n",
        "        # Format results\n",
        "        formatted_results = []\n",
        "        if results['documents'] and len(results['documents'][0]) > 0:\n",
        "            for i, (doc, metadata, distance) in enumerate(zip(\n",
        "                results['documents'][0],\n",
        "                results['metadatas'][0],\n",
        "                results['distances'][0]\n",
        "            )):\n",
        "                formatted_results.append({\n",
        "                    \"id\": results['ids'][0][i],\n",
        "                    \"content\": doc,\n",
        "                    \"metadata\": metadata,\n",
        "                    \"parent_doc_id\": metadata.get(\"parent_doc_id\", \"unknown\"),\n",
        "                    \"similarity\": 1 - distance,  # Convert distance to similarity\n",
        "                    \"distance\": distance\n",
        "                })\n",
        "\n",
        "        return formatted_results\n",
        "\n",
        "    def get_collection_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get collection statistics\"\"\"\n",
        "        if not self.collection:\n",
        "            return {\"error\": \"Collection not initialized\"}\n",
        "\n",
        "        count = self.collection.count()\n",
        "\n",
        "        return {\n",
        "            \"total_documents\": count,\n",
        "        }\n",
        "\n",
        "    def delete_documents(self, parent_doc_id: Optional[str] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Delete documents by parent document ID\"\"\" # Updated description\n",
        "        try:\n",
        "            if not self.collection:\n",
        "                return {\"error\": \"ChromaDB not initialized\"}\n",
        "\n",
        "            where_clause = {}\n",
        "            if parent_doc_id:\n",
        "                where_clause[\"parent_doc_id\"] = {\"$eq\": parent_doc_id}\n",
        "\n",
        "            if not where_clause:\n",
        "                return {\"error\": \"Must specify parent_doc_id\"} # Updated error message\n",
        "\n",
        "            # Get documents to delete\n",
        "            to_delete = self.collection.get(\n",
        "                where=where_clause,\n",
        "                include=[\"metadatas\"]\n",
        "            )\n",
        "\n",
        "            if to_delete['ids']:\n",
        "                self.collection.delete(ids=to_delete['ids'])\n",
        "                return {\"status\": \"success\", \"deleted_count\": len(to_delete['ids'])}\n",
        "            else:\n",
        "                return {\"status\": \"success\", \"deleted_count\": 0, \"message\": \"No documents found matching criteria\"}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error deleting documents: {str(e)}\")\n",
        "            return {\"error\": str(e)}"
      ],
      "metadata": {
        "id": "R8WVl_2ifwq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChromaRetriever:\n",
        "    \"\"\"Custom retriever for LangChain integration with ChromaDB\"\"\"\n",
        "\n",
        "    def __init__(self, vector_store: ChromaVectorStore, k: int = 5):\n",
        "        self.vector_store = vector_store\n",
        "        self.k = k\n",
        "        # self.classification_filter = classification_filter # Removed classification_filter\n",
        "\n",
        "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        \"\"\"Retrieve relevant documents\"\"\"\n",
        "        results = self.vector_store.similarity_search(\n",
        "            query=query,\n",
        "            k=self.k,\n",
        "            # classification_filter=self.classification_filter # Removed classification_filter\n",
        "        )\n",
        "\n",
        "        documents = []\n",
        "        for result in results:\n",
        "            doc = Document(\n",
        "                page_content=result[\"content\"],\n",
        "                metadata={\n",
        "                    **result[\"metadata\"],\n",
        "                    # \"classification\": result[\"classification\"], # Removed classification\n",
        "                    \"similarity\": result[\"similarity\"],\n",
        "                    \"parent_doc_id\": result[\"parent_doc_id\"]\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "\n",
        "        return documents"
      ],
      "metadata": {
        "id": "RAoTfoXcf1lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete RAG system orchestrator with ChromaDB\"\"\"\n",
        "\n",
        "    def __init__(self, persist_directory: str = \"./chroma_db\", collection_name: str = \"rag_documents\"):\n",
        "        self.processor = DocumentProcessor()\n",
        "        self.embedder = ClasswiseEmbedder()\n",
        "        self.vector_store = ChromaVectorStore(persist_directory, collection_name)\n",
        "        self.llm = Ollama(model=\"llama3\")\n",
        "        self.retriever = None\n",
        "        self.qa_chain = None\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize the RAG system\"\"\"\n",
        "        self.vector_store.initialize()\n",
        "        logger.info(\"RAG system initialized with ChromaDB\")\n",
        "\n",
        "    def ingest_document(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete document ingestion pipeline\"\"\"\n",
        "        try:\n",
        "            # Parse document\n",
        "            doc = self.processor.parse_document(file_path)\n",
        "\n",
        "            # Chunk document\n",
        "            chunks = self.processor.chunk_document(doc)\n",
        "\n",
        "            # Generate embeddings\n",
        "            embedded_chunks = self.embedder.embed_chunks(chunks)\n",
        "\n",
        "            # Store in vector database\n",
        "            self.vector_store.add_embeddings(embedded_chunks)\n",
        "\n",
        "            logger.info(f\"Successfully ingested document: {file_path}\")\n",
        "            return {\"status\": \"success\", \"chunks_created\": len(chunks)}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting document {file_path}: {str(e)}\")\n",
        "            return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "    def query(self, question: str, k: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"Query the RAG system\"\"\"\n",
        "        try:\n",
        "            # Setup retriever\n",
        "            retriever = ChromaRetriever(\n",
        "                vector_store=self.vector_store,\n",
        "                k=k,\n",
        "            )\n",
        "\n",
        "            # Get relevant documents\n",
        "            docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "            if not docs:\n",
        "                return {\n",
        "                    \"answer\": \"No relevant documents found.\",\n",
        "                    \"sources\": [],\n",
        "                }\n",
        "\n",
        "            # Create context from retrieved documents\n",
        "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            # Generate answer using LLAMA3\n",
        "            prompt = f\"\"\"Based on the following context, answer the question. If the answer cannot be found in the context, say so clearly.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "            answer = self.llm(prompt)\n",
        "\n",
        "            # Prepare sources\n",
        "            sources = []\n",
        "            for doc in docs:\n",
        "                sources.append({\n",
        "                    \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                    \"similarity\": doc.metadata.get(\"similarity\"),\n",
        "                    \"file_name\": doc.metadata.get(\"file_name\")\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"sources\": sources,\n",
        "                \"num_sources\": len(sources)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error querying RAG system: {str(e)}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system statistics\"\"\"\n",
        "        try:\n",
        "            return self.vector_store.get_collection_stats()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting stats: {str(e)}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def delete_documents(self, parent_doc_id: Optional[str] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Delete documents by parent document ID\"\"\"\n",
        "        try:\n",
        "            if not self.vector_store.collection:\n",
        "                return {\"error\": \"ChromaDB not initialized\"}\n",
        "\n",
        "            where_clause = {}\n",
        "            if parent_doc_id:\n",
        "                where_clause[\"parent_doc_id\"] = {\"$eq\": parent_doc_id}\n",
        "\n",
        "            if not where_clause:\n",
        "                return {\"error\": \"Must specify parent_doc_id\"}\n",
        "\n",
        "            # Get documents to delete\n",
        "            to_delete = self.vector_store.collection.get(\n",
        "                where=where_clause,\n",
        "                include=[\"metadatas\"]\n",
        "            )\n",
        "\n",
        "            if to_delete['ids']:\n",
        "                self.vector_store.collection.delete(ids=to_delete['ids'])\n",
        "                return {\"status\": \"success\", \"deleted_count\": len(to_delete['ids'])}\n",
        "            else:\n",
        "                return {\"status\": \"success\", \"deleted_count\": 0, \"message\": \"No documents found matching criteria\"}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error deleting documents: {str(e)}\")\n",
        "            return {\"error\": str(e)}"
      ],
      "metadata": {
        "id": "iOVYUTFHf4aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MCPRAGServer:\n",
        "    \"\"\"MCP Server for the RAG system with ChromaDB\"\"\"\n",
        "\n",
        "    def __init__(self, persist_directory: str = \"./chroma_db\"):\n",
        "        self.rag_system = RAGSystem(persist_directory=persist_directory)\n",
        "        self.server = Server(\"rag-chroma-server\")\n",
        "        self._setup_handlers()\n",
        "\n",
        "    def _setup_handlers(self):\n",
        "        \"\"\"Setup MCP server handlers\"\"\"\n",
        "\n",
        "        @self.server.list_resources()\n",
        "        def list_resources() -> List[Resource]:\n",
        "            \"\"\"List available resources\"\"\"\n",
        "            return [\n",
        "                Resource(\n",
        "                    uri=\"rag://stats\",\n",
        "                    name=\"RAG System Statistics\",\n",
        "                    mimeType=\"application/json\",\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "        @self.server.read_resource()\n",
        "        def read_resource(uri: str) -> str:\n",
        "            \"\"\"Read resource content\"\"\"\n",
        "            if uri == \"rag://stats\":\n",
        "                stats = self.rag_system.get_stats()\n",
        "                return json.dumps(stats, indent=2)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown resource: {uri}\")\n",
        "\n",
        "        @self.server.list_tools()\n",
        "        def list_tools() -> List[Tool]:\n",
        "            \"\"\"List available tools\"\"\"\n",
        "            return [\n",
        "                Tool(\n",
        "                    name=\"ingest_document\",\n",
        "                    description=\"Ingest a document into the RAG system\",\n",
        "                    inputSchema={\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"file_path\": {\"type\": \"string\", \"description\": \"Path to the document file\"},\n",
        "                        },\n",
        "                        \"required\": [\"file_path\"]\n",
        "                    }\n",
        "                ),\n",
        "                Tool(\n",
        "                    name=\"query_rag\",\n",
        "                    description=\"Query the RAG system\",\n",
        "                    inputSchema={\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"question\": {\"type\": \"string\", \"description\": \"Question to ask\"},\n",
        "                            \"k\": {\"type\": \"integer\", \"description\": \"Number of documents to retrieve\", \"default\": 5}\n",
        "                        },\n",
        "                        \"required\": [\"question\"]\n",
        "                    }\n",
        "                ),\n",
        "                Tool(\n",
        "                    name=\"get_system_stats\",\n",
        "                    description=\"Get RAG system statistics\",\n",
        "                    inputSchema={\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {}\n",
        "                    }\n",
        "                ),\n",
        "                Tool(\n",
        "                    name=\"delete_documents\",\n",
        "                    description=\"Delete documents by parent document ID\",\n",
        "                    inputSchema={\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"parent_doc_id\": {\"type\": \"string\", \"description\": \"Delete all chunks of this parent document\"}\n",
        "                        }\n",
        "                    }\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "        @self.server.call_tool()\n",
        "        def call_tool(name: str, arguments: dict) -> List[Any]:\n",
        "            \"\"\"Handle tool calls\"\"\"\n",
        "            if name == \"ingest_document\":\n",
        "                result = self.rag_system.ingest_document(\n",
        "                    file_path=arguments[\"file_path\"],\n",
        "                )\n",
        "                return [{\"result\": result}]\n",
        "\n",
        "            elif name == \"query_rag\":\n",
        "                result = self.rag_system.query(\n",
        "                    question=arguments[\"question\"],\n",
        "                    k=arguments.get(\"k\", 5)\n",
        "                )\n",
        "                return [{\"result\": result}]\n",
        "\n",
        "            elif name == \"get_system_stats\":\n",
        "                result = self.rag_system.get_stats()\n",
        "                return [{\"result\": result}]\n",
        "\n",
        "            elif name == \"delete_documents\":\n",
        "                result = self.rag_system.delete_documents(\n",
        "                    parent_doc_id=arguments.get(\"parent_doc_id\")\n",
        "                )\n",
        "                return [{\"result\": result}]\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Run the MCP server\"\"\"\n",
        "        self.rag_system.initialize()\n",
        "\n",
        "        # For synchronous operation in environments that don't support async MCP\n",
        "        import mcp.server.stdio as stdio\n",
        "        stdio.run_server(self.server)"
      ],
      "metadata": {
        "id": "EjlkQqA9gARk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Utility functions for Jupyter notebook usage\n",
        "def create_sample_documents():\n",
        "    \"\"\"Create sample documents for testing\"\"\"\n",
        "    sample_docs = [\n",
        "        {\n",
        "            \"filename\": \"tech_doc.txt\",\n",
        "            \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions without being explicitly programmed. It uses algorithms to analyze data, identify patterns, and make predictions or classifications.\",\n",
        "            \"classification\": \"technical\",\n",
        "            \"confidence\": 0.95\n",
        "        },\n",
        "        {\n",
        "            \"filename\": \"business_doc.txt\",\n",
        "            \"content\": \"Our quarterly revenue has increased by 15% compared to last year. The marketing team has successfully launched three new campaigns, resulting in improved customer acquisition and retention rates.\",\n",
        "            \"classification\": \"business\",\n",
        "            \"confidence\": 0.88\n",
        "        },\n",
        "        {\n",
        "            \"filename\": \"research_doc.txt\",\n",
        "            \"content\": \"The study examined the effects of climate change on marine ecosystems. Results showed a significant correlation between rising sea temperatures and coral bleaching events across multiple reef systems.\",\n",
        "            \"classification\": \"research\",\n",
        "            \"confidence\": 0.92\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Create sample files\n",
        "    for doc in sample_docs:\n",
        "        with open(doc[\"filename\"], \"w\") as f:\n",
        "            f.write(doc[\"content\"])\n",
        "\n",
        "    return sample_docs\n"
      ],
      "metadata": {
        "id": "DyYp2KtCgEro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage and testing\n",
        "def main():\n",
        "    \"\"\"Example usage of the RAG system\"\"\"\n",
        "    # Initialize RAG system\n",
        "    rag = RAGSystem(persist_directory=\"./test_chroma_db\")\n",
        "    rag.initialize()\n",
        "\n",
        "    # Define your actual documents here\n",
        "    actual_documents = [\n",
        "        {\n",
        "            \"file_path\": \"/content/business_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        {\n",
        "            \"file_path\": \"/content/engineering_drawing_specifications.pdf\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/maintenance_job_card.pdf\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/research_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/tech_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "    ]\n",
        "\n",
        "    print(\"Ingesting actual documents...\")\n",
        "    for doc_info in actual_documents:\n",
        "        result = rag.ingest_document(doc_info[\"file_path\"]) # Removed classification and confidence\n",
        "        print(f\"Ingested {doc_info['file_path']}: {result}\")\n",
        "\n",
        "    # Show stats\n",
        "    print(f\"\\nSystem Stats: {rag.get_stats()}\")\n",
        "\n",
        "\n",
        "    # Interactive querying\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Interactive RAG Querying\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enter your questions. Type 'quit' or 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \")\n",
        "        if question.lower() in [\"quit\", \"exit\"]:\n",
        "            break\n",
        "\n",
        "        print(\"Processing your question...\")\n",
        "        result = rag.query(question)\n",
        "\n",
        "        print(f\"Answer: {result.get('answer', 'No answer')}\")\n",
        "        sources = result.get('sources', [])\n",
        "        print(f\"Sources Found: {len(sources)}\")\n",
        "        if sources:\n",
        "            print(\"--- Sources ---\")\n",
        "            for i, source in enumerate(sources):\n",
        "                print(f\"Source {i+1}: {source.get('file_name', 'Unknown File')} (Similarity: {source.get('similarity', 'N/A'):.4f})\")\n",
        "                print(f\"Content: {source.get('content', 'N/A')}\")\n",
        "                print(\"-\" * 10)\n",
        "            print(\"---------------\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7pZ5r84gHt-",
        "outputId": "956043fa-23b4-4222-e818-b9935c799c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingesting actual documents...\n",
            "Ingested /content/business_doc.txt: {'status': 'success', 'chunks_created': 1}\n",
            "Ingested /content/engineering_drawing_specifications.pdf: {'status': 'success', 'chunks_created': 10}\n",
            "Ingested /content/maintenance_job_card.pdf: {'status': 'success', 'chunks_created': 11}\n",
            "Ingested /content/research_doc.txt: {'status': 'success', 'chunks_created': 1}\n",
            "Ingested /content/tech_doc.txt: {'status': 'success', 'chunks_created': 1}\n",
            "\n",
            "System Stats: {'total_documents': 24}\n",
            "\n",
            "==================================================\n",
            "Interactive RAG Querying\n",
            "==================================================\n",
            "Enter your questions. Type 'quit' or 'exit' to stop.\n",
            "\n",
            "Your question: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyuXuBz0gKT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24c86e79"
      },
      "source": [
        "# Task\n",
        "Integrate the provided Python code for interacting with the Ollama API into the existing RAG system codebase to use the `llama3-chatqa` model for generating answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964388b3"
      },
      "source": [
        "## Create a custom ollama llm class\n",
        "\n",
        "### Subtask:\n",
        "Define a new Python class `CustomOllamaLLM` that inherits from a Langchain BaseLLM or similar base class. This class will encapsulate the logic for interacting with the Ollama API using the provided `query_ollama_api` or `chat_with_ollama` functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9c88c0"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `CustomOllamaLLM` class inheriting from `BaseLLM` and include the necessary methods for interacting with the Ollama API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64d4715e"
      },
      "source": [
        "from langchain.llms.base import BaseLLM\n",
        "from typing import Any, List, Mapping, Optional\n",
        "import requests\n",
        "\n",
        "async def query_ollama_api(api_url: str, model_name: str, prompt: str) -> str:\n",
        "    \"\"\"Queries the Ollama API for a text generation.\"\"\"\n",
        "    url = f\"{api_url}/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        result = response.json()\n",
        "        return result.get(\"response\", \"\").strip()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error querying Ollama API: {e}\")\n",
        "        return f\"Error querying Ollama API: {e}\"\n",
        "\n",
        "# Although the original code included chat_with_ollama, the RAG query logic currently uses a single prompt for the LLM.\n",
        "# We will adapt the query_ollama_api function for the CustomOllamaLLM to fit the current RAG query structure.\n",
        "# If chat functionality is required later, we can integrate it.\n",
        "\n",
        "class CustomOllamaLLM(BaseLLM):\n",
        "    \"\"\"Custom Langchain LLM for interacting with the Ollama API.\"\"\"\n",
        "\n",
        "    ollama_url: str\n",
        "    model_name: str = \"llama3-chatqa\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_ollama\"\n",
        "\n",
        "    def __init__(self, ollama_url: str = \"http://localhost:11434\", model_name: str = \"llama3-chatqa\", **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        self.ollama_url = ollama_url\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call the Ollama API to generate text.\"\"\"\n",
        "        # Langchain's _call is synchronous, so we'll use the synchronous requests call\n",
        "        # If async is strictly needed, we would use BaseChatModel and its async methods\n",
        "        url = f\"{self.ollama_url}/api/generate\"\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        try:\n",
        "            response = requests.post(url, json=payload)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            return result.get(\"response\", \"\").strip()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Error in CustomOllamaLLM call: {e}\")\n",
        "            return f\"Error generating response: {e}\"\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _identifyingparams(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\"ollama_url\": self.ollama_url, \"model_name\": self.model_name}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a08fb5cf"
      },
      "source": [
        "## Implement the ` call` or ` generate` method\n",
        "\n",
        "### Subtask:\n",
        "Within the custom LLM class, implement the method required by the Langchain base class (usually `_call` for simple LLMs or `_generate` for more complex ones) to send the prompt to the Ollama API and return the generated response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0545dfcc"
      },
      "source": [
        "## Integrate the custom llm into ragsystem\n",
        "\n",
        "### Subtask:\n",
        "Modify the `RAGSystem` class to use an instance of the new custom Ollama LLM class instead of the existing `Ollama(model=\"llama3\")`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07b698ba"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the RAGSystem class to use the CustomOllamaLLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec258106"
      },
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete RAG system orchestrator with ChromaDB\"\"\"\n",
        "\n",
        "    def __init__(self, persist_directory: str = \"./chroma_db\", collection_name: str = \"rag_documents\"):\n",
        "        self.processor = DocumentProcessor()\n",
        "        self.embedder = ClasswiseEmbedder()\n",
        "        self.vector_store = ChromaVectorStore(persist_directory, collection_name)\n",
        "        # Use the custom Ollama LLM\n",
        "        self.llm = CustomOllamaLLM(ollama_url=\"http://localhost:11434\", model_name=\"llama3-chatqa\")\n",
        "        self.retriever = None\n",
        "        self.qa_chain = None\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize the RAG system\"\"\"\n",
        "        self.vector_store.initialize()\n",
        "        logger.info(\"RAG system initialized with ChromaDB\")\n",
        "\n",
        "    def ingest_document(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete document ingestion pipeline\"\"\"\n",
        "        try:\n",
        "            # Parse document\n",
        "            doc = self.processor.parse_document(file_path)\n",
        "\n",
        "            # Chunk document\n",
        "            chunks = self.processor.chunk_document(doc)\n",
        "\n",
        "            # Generate embeddings\n",
        "            embedded_chunks = self.embedder.embed_chunks(chunks)\n",
        "\n",
        "            # Store in vector database\n",
        "            self.vector_store.add_embeddings(embedded_chunks)\n",
        "\n",
        "            logger.info(f\"Successfully ingested document: {file_path}\")\n",
        "            return {\"status\": \"success\", \"chunks_created\": len(chunks)}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting document {file_path}: {str(e)}\")\n",
        "            return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "    def query(self, question: str, k: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"Query the RAG system\"\"\"\n",
        "        try:\n",
        "            # Setup retriever\n",
        "            retriever = ChromaRetriever(\n",
        "                vector_store=self.vector_store,\n",
        "                k=k,\n",
        "            )\n",
        "\n",
        "            # Get relevant documents\n",
        "            docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "            if not docs:\n",
        "                return {\n",
        "                    \"answer\": \"No relevant documents found.\",\n",
        "                    \"sources\": [],\n",
        "                }\n",
        "\n",
        "            # Create context from retrieved documents\n",
        "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            # Generate answer using LLAMA3\n",
        "            prompt = f\"\"\"Based on the following context, answer the question. If the answer cannot be found in the context, say so clearly.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "            answer = self.llm(prompt)\n",
        "\n",
        "            # Prepare sources\n",
        "            sources = []\n",
        "            for doc in docs:\n",
        "                sources.append({\n",
        "                    \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                    \"similarity\": doc.metadata.get(\"similarity\"),\n",
        "                    \"file_name\": doc.metadata.get(\"file_name\")\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"sources\": sources,\n",
        "                \"num_sources\": len(sources)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error querying RAG system: {str(e)}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system statistics\"\"\"\n",
        "        try:\n",
        "            return self.vector_store.get_collection_stats()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting stats: {str(e)}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def delete_documents(self, parent_doc_id: Optional[str] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Delete documents by parent document ID\"\"\"\n",
        "        try:\n",
        "            if not self.vector_store.collection:\n",
        "                return {\"error\": \"ChromaDB not initialized\"}\n",
        "\n",
        "            where_clause = {}\n",
        "            if parent_doc_id:\n",
        "                where_clause[\"parent_doc_id\"] = {\"$eq\": parent_doc_id}\n",
        "\n",
        "            if not where_clause:\n",
        "                return {\"error\": \"Must specify parent_doc_id\"}\n",
        "\n",
        "            # Get documents to delete\n",
        "            to_delete = self.vector_store.collection.get(\n",
        "                where=where_clause,\n",
        "                include=[\"metadatas\"]\n",
        "            )\n",
        "\n",
        "            if to_delete['ids']:\n",
        "                self.vector_store.collection.delete(ids=to_delete['ids'])\n",
        "                return {\"status\": \"success\", \"deleted_count\": len(to_delete['ids'])}\n",
        "            else:\n",
        "                return {\"status\": \"success\", \"deleted_count\": 0, \"message\": \"No documents found matching criteria\"}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error deleting documents: {str(e)}\")\n",
        "            return {\"error\": str(e)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca4eb175"
      },
      "source": [
        "## Update the `main` function (if necessary)\n",
        "\n",
        "### Subtask:\n",
        "Ensure the `main` function correctly initializes and uses the `RAGSystem` with the new custom LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfc2efe"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the `main` function to confirm it initializes and uses the RAGSystem as expected after the previous modification. No code changes are anticipated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "bbf607f1",
        "outputId": "6846ba82-2998-4a8c-e46f-fa3f1066361b"
      },
      "source": [
        "# Example usage and testing\n",
        "def main():\n",
        "    \"\"\"Example usage of the RAG system\"\"\"\n",
        "    # Initialize RAG system\n",
        "    # The RAGSystem class now internally uses CustomOllamaLLM with llama3-chatqa\n",
        "    rag = RAGSystem(persist_directory=\"./test_chroma_db\")\n",
        "    rag.initialize()\n",
        "\n",
        "    # Define your actual documents here\n",
        "    actual_documents = [\n",
        "        {\n",
        "            \"file_path\": \"/content/business_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        {\n",
        "            \"file_path\": \"/content/engineering_drawing_specifications.pdf\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/maintenance_job_card.pdf\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/research_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/tech_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "    ]\n",
        "\n",
        "    print(\"Ingesting actual documents...\")\n",
        "    for doc_info in actual_documents:\n",
        "        # These calls are compatible with the updated RAGSystem\n",
        "        result = rag.ingest_document(doc_info[\"file_path\"])\n",
        "        print(f\"Ingested {doc_info['file_path']}: {result}\")\n",
        "\n",
        "    # Show stats\n",
        "    print(f\"\\nSystem Stats: {rag.get_stats()}\")\n",
        "\n",
        "\n",
        "    # Interactive querying\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Interactive RAG Querying\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enter your questions. Type 'quit' or 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \")\n",
        "        if question.lower() in [\"quit\", \"exit\"]:\n",
        "            break\n",
        "\n",
        "        print(\"Processing your question...\")\n",
        "        # These calls are compatible with the updated RAGSystem\n",
        "        result = rag.query(question)\n",
        "\n",
        "        print(f\"Answer: {result.get('answer', 'No answer')}\")\n",
        "        sources = result.get('sources', [])\n",
        "        print(f\"Sources Found: {len(sources)}\")\n",
        "        if sources:\n",
        "            print(\"--- Sources ---\")\n",
        "            for i, source in enumerate(sources):\n",
        "                print(f\"Source {i+1}: {source.get('file_name', 'Unknown File')} (Similarity: {source.get('similarity', 'N/A'):.4f})\")\n",
        "                print(f\"Content: {source.get('content', 'N/A')}\")\n",
        "                print(\"-\" * 10)\n",
        "            print(\"---------------\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Can't instantiate abstract class CustomOllamaLLM without an implementation for abstract method '_generate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2843286620.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2843286620.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Initialize RAG system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# The RAGSystem class now internally uses CustomOllamaLLM with llama3-chatqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./test_chroma_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mrag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2638983820.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, persist_directory, collection_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChromaVectorStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Use the custom Ollama LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomOllamaLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mollama_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"http://localhost:11434\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3-chatqa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqa_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class CustomOllamaLLM without an implementation for abstract method '_generate'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b0120c5"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `CustomOllamaLLM` needs to implement the abstract method `_generate`. I need to modify the `CustomOllamaLLM` class to implement `_generate` instead of or in addition to `_call` to satisfy the requirements of the Langchain `BaseLLM`. I will update the class definition to include the `_generate` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0IgVP7roqN8"
      },
      "source": [
        "from langchain.llms.base import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from typing import Any, List, Mapping, Optional\n",
        "import requests\n",
        "import asyncio # Import asyncio for potential async operations if needed later\n",
        "\n",
        "async def query_ollama_api(api_url: str, model_name: str, prompt: str) -> str:\n",
        "    \"\"\"Queries the Ollama API for a text generation.\"\"\"\n",
        "    url = f\"{api_url}/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    try:\n",
        "        # Use asyncio for awaited operations if the context supports it\n",
        "        # For now, keep the synchronous requests call within _generate\n",
        "        # If async is required, this function would need to be used within an async context\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        result = response.json()\n",
        "        return result.get(\"response\", \"\").strip()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error querying Ollama API: {e}\")\n",
        "        return f\"Error querying Ollama API: {e}\"\n",
        "\n",
        "\n",
        "class CustomOllamaLLM(BaseLLM):\n",
        "    \"\"\"Custom Langchain LLM for interacting with the Ollama API.\"\"\"\n",
        "\n",
        "    ollama_url: str\n",
        "    model_name: str = \"llama3-chatqa\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_ollama\"\n",
        "\n",
        "    def __init__(self, ollama_url: str = \"http://localhost:11434\", model_name: str = \"llama3-chatqa\", **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        self.ollama_url = ollama_url\n",
        "        self.model_name = model_name\n",
        "\n",
        "    # Implement the _generate method as required by the abstract base class\n",
        "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None) -> LLMResult:\n",
        "        \"\"\"Generate text based on the provided prompts.\"\"\"\n",
        "        # This method is expected to handle a list of prompts and return an LLMResult\n",
        "        # For simplicity, we'll process each prompt individually for now\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop=stop) # Reuse the existing _call logic\n",
        "            generations.append([{\"text\": text}]) # Langchain expects a list of lists of generations\n",
        "\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    # Keep _call for internal use or backward compatibility if needed, though _generate is the primary method\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call the Ollama API to generate text (synchronous helper).\"\"\"\n",
        "        url = f\"{self.ollama_url}/api/generate\"\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        try:\n",
        "            response = requests.post(url, json=payload)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            return result.get(\"response\", \"\").strip()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Error in CustomOllamaLLM call: {e}\")\n",
        "            return f\"Error generating response: {e}\"\n",
        "\n",
        "    @property\n",
        "    def _identifyingparams(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\"ollama_url\": self.ollama_url, \"model_name\": self.model_name}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1460f036"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `CustomOllamaLLM` class has the required `_generate` method, I need to re-run the `main` function code block to verify that the `RAGSystem` can be initialized and used without the `TypeError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "a30c8632",
        "outputId": "a4334fe9-6908-4eed-d38f-352730414e98"
      },
      "source": [
        "# Example usage and testing\n",
        "def main():\n",
        "    \"\"\"Example usage of the RAG system\"\"\"\n",
        "    # Initialize RAG system\n",
        "    # The RAGSystem class now internally uses CustomOllamaLLM with llama3-chatqa\n",
        "    rag = RAGSystem(persist_directory=\"./test_chroma_db\")\n",
        "    rag.initialize()\n",
        "\n",
        "    # Define your actual documents here\n",
        "    actual_documents = [\n",
        "        {\n",
        "            \"file_path\": \"/content/business_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        {\n",
        "            \"file_path\": \"/content/engineering_drawing_specifications.pdf\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/maintenance_job_card.pdf\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/research_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/tech_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "    ]\n",
        "\n",
        "    print(\"Ingesting actual documents...\")\n",
        "    for doc_info in actual_documents:\n",
        "        # These calls are compatible with the updated RAGSystem\n",
        "        result = rag.ingest_document(doc_info[\"file_path\"])\n",
        "        print(f\"Ingested {doc_info['file_path']}: {result}\")\n",
        "\n",
        "    # Show stats\n",
        "    print(f\"\\nSystem Stats: {rag.get_stats()}\")\n",
        "\n",
        "\n",
        "    # Interactive querying\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Interactive RAG Querying\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enter your questions. Type 'quit' or 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \")\n",
        "        if question.lower() in [\"quit\", \"exit\"]:\n",
        "            break\n",
        "\n",
        "        print(\"Processing your question...\")\n",
        "        # These calls are compatible with the updated RAGSystem\n",
        "        result = rag.query(question)\n",
        "\n",
        "        print(f\"Answer: {result.get('answer', 'No answer')}\")\n",
        "        sources = result.get('sources', [])\n",
        "        print(f\"Sources Found: {len(sources)}\")\n",
        "        if sources:\n",
        "            print(\"--- Sources ---\")\n",
        "            for i, source in enumerate(sources):\n",
        "                print(f\"Source {i+1}: {source.get('file_name', 'Unknown File')} (Similarity: {source.get('similarity', 'N/A'):.4f})\")\n",
        "                print(f\"Content: {source.get('content', 'N/A')}\")\n",
        "                print(\"-\" * 10)\n",
        "            print(\"---------------\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for CustomOllamaLLM\nollama_url\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2843286620.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2843286620.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Initialize RAG system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# The RAGSystem class now internally uses CustomOllamaLLM with llama3-chatqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./test_chroma_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mrag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2638983820.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, persist_directory, collection_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChromaVectorStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Use the custom Ollama LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomOllamaLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mollama_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"http://localhost:11434\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3-chatqa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqa_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3987115668.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ollama_url, model_name, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mollama_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://localhost:11434\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"llama3-chatqa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mollama_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mollama_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for CustomOllamaLLM\nollama_url\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5e0102"
      },
      "source": [
        "**Reasoning**:\n",
        "The validation error indicates that the `ollama_url` field is required during initialization, but it's not being passed correctly to the `super().__init__` call in `CustomOllamaLLM`. I need to pass the `ollama_url` and `model_name` to the parent class constructor as part of the `kwargs`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcTYz6jpoxku"
      },
      "source": [
        "from langchain.llms.base import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from typing import Any, List, Mapping, Optional\n",
        "import requests\n",
        "import asyncio # Import asyncio for potential async operations if needed later\n",
        "\n",
        "async def query_ollama_api(api_url: str, model_name: str, prompt: str) -> str:\n",
        "    \"\"\"Queries the Ollama API for a text generation.\"\"\"\n",
        "    url = f\"{api_url}/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        result = response.json()\n",
        "        return result.get(\"response\", \"\").strip()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error querying Ollama API: {e}\")\n",
        "        return f\"Error querying Ollama API: {e}\"\n",
        "\n",
        "\n",
        "class CustomOllamaLLM(BaseLLM):\n",
        "    \"\"\"Custom Langchain LLM for interacting with the Ollama API.\"\"\"\n",
        "\n",
        "    ollama_url: str\n",
        "    model_name: str = \"llama3-chatqa\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_ollama\"\n",
        "\n",
        "    def __init__(self, ollama_url: str = \"http://localhost:11434\", model_name: str = \"llama3-chatqa\", **kwargs: Any):\n",
        "        # Pass ollama_url and model_name to the parent class constructor\n",
        "        super().__init__(ollama_url=ollama_url, model_name=model_name, **kwargs)\n",
        "        self.ollama_url = ollama_url\n",
        "        self.model_name = model_name\n",
        "\n",
        "\n",
        "    # Implement the _generate method as required by the abstract base class\n",
        "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None) -> LLMResult:\n",
        "        \"\"\"Generate text based on the provided prompts.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop=stop) # Reuse the existing _call logic\n",
        "            generations.append([{\"text\": text}]) # Langchain expects a list of lists of generations\n",
        "\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    # Keep _call for internal use or backward compatibility if needed, though _generate is the primary method\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call the Ollama API to generate text (synchronous helper).\"\"\"\n",
        "        url = f\"{self.ollama_url}/api/generate\"\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        try:\n",
        "            response = requests.post(url, json=payload)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            return result.get(\"response\", \"\").strip()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Error in CustomOllamaLLM call: {e}\")\n",
        "            return f\"Error generating response: {e}\"\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _identifyingparams(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\"ollama_url\": self.ollama_url, \"model_name\": self.model_name}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "530407dc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error was a `ValidationError` because the required fields were not passed to the parent class constructor in `CustomOllamaLLM`. I have corrected the `__init__` method in the `CustomOllamaLLM` class to pass `ollama_url` and `model_name` to `super().__init__`. Now I need to re-run the `main` function code block to verify that this change resolves the validation error and the `RAGSystem` can be initialized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f57ddee0",
        "outputId": "02a2f005-46bb-4f84-b98b-984d3550db3b"
      },
      "source": [
        "# Example usage and testing\n",
        "def main():\n",
        "    \"\"\"Example usage of the RAG system\"\"\"\n",
        "    # Initialize RAG system\n",
        "    # The RAGSystem class now internally uses CustomOllamaLLM with llama3-chatqa\n",
        "    rag = RAGSystem(persist_directory=\"./test_chroma_db\")\n",
        "    rag.initialize()\n",
        "\n",
        "    # Define your actual documents here\n",
        "    actual_documents = [\n",
        "        {\n",
        "            \"file_path\": \"/content/business_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        {\n",
        "            \"file_path\": \"/content/engineering_drawing_specifications.pdf\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/maintenance_job_card.pdf\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/research_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "        {\n",
        "            \"file_path\": \"/content/tech_doc.txt\", # Replace with your actual file path\n",
        "        },\n",
        "        # Add more documents as needed\n",
        "    ]\n",
        "\n",
        "    print(\"Ingesting actual documents...\")\n",
        "    for doc_info in actual_documents:\n",
        "        # These calls are compatible with the updated RAGSystem\n",
        "        result = rag.ingest_document(doc_info[\"file_path\"])\n",
        "        print(f\"Ingested {doc_info['file_path']}: {result}\")\n",
        "\n",
        "    # Show stats\n",
        "    print(f\"\\nSystem Stats: {rag.get_stats()}\")\n",
        "\n",
        "\n",
        "    # Interactive querying\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Interactive RAG Querying\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enter your questions. Type 'quit' or 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \")\n",
        "        if question.lower() in [\"quit\", \"exit\"]:\n",
        "            break\n",
        "\n",
        "        print(\"Processing your question...\")\n",
        "        # These calls are compatible with the updated RAGSystem\n",
        "        result = rag.query(question)\n",
        "\n",
        "        print(f\"Answer: {result.get('answer', 'No answer')}\")\n",
        "        sources = result.get('sources', [])\n",
        "        print(f\"Sources Found: {len(sources)}\")\n",
        "        if sources:\n",
        "            print(\"--- Sources ---\")\n",
        "            for i, source in enumerate(sources):\n",
        "                print(f\"Source {i+1}: {source.get('file_name', 'Unknown File')} (Similarity: {source.get('similarity', 'N/A'):.4f})\")\n",
        "                print(f\"Content: {source.get('content', 'N/A')}\")\n",
        "                print(\"-\" * 10)\n",
        "            print(\"---------------\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingesting actual documents...\n",
            "Ingested /content/business_doc.txt: {'status': 'success', 'chunks_created': 1}\n",
            "Ingested /content/engineering_drawing_specifications.pdf: {'status': 'success', 'chunks_created': 10}\n",
            "Ingested /content/maintenance_job_card.pdf: {'status': 'success', 'chunks_created': 11}\n",
            "Ingested /content/research_doc.txt: {'status': 'success', 'chunks_created': 1}\n",
            "Ingested /content/tech_doc.txt: {'status': 'success', 'chunks_created': 1}\n",
            "\n",
            "System Stats: {'total_documents': 24}\n",
            "\n",
            "==================================================\n",
            "Interactive RAG Querying\n",
            "==================================================\n",
            "Enter your questions. Type 'quit' or 'exit' to stop.\n",
            "\n",
            "Your question: tell me about kochi metro\n",
            "Processing your question...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error in CustomOllamaLLM call: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b45d2a56690>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Error generating response: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b45d2a56690>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Sources Found: 5\n",
            "--- Sources ---\n",
            "Source 1: maintenance_job_card.pdf (Similarity: 0.1048)\n",
            "Content: /Info 8 0 R\n",
            "/Root 7 0 R\n",
            "/Size 13\n",
            ">>\n",
            "startxref\n",
            "5383\n",
            "%%EOF\n",
            "----------\n",
            "Source 2: engineering_drawing_specifications.pdf (Similarity: 0.1011)\n",
            "Content: /Info 9 0 R\n",
            "/Root 8 0 R\n",
            "/Size 14\n",
            ">>\n",
            "startxref\n",
            "5011\n",
            "%%EOF\n",
            "----------\n",
            "Source 3: maintenance_job_card.pdf (Similarity: 0.0864)\n",
            "Content: Gat=+D,]1Y&H9tYfL\\8k\\6.tF2LJ/KKMFmh`(HquiQM%EZEHfClCA&X`]q2L-e[.4,A)l8$ZXmeB:jTq*Is3S_#<C!^i(ZlIg@/&+jD-c]bo0nE0HmkHP1EJ`tK<K%kmFDn*ZU_#sK,@/;5<>Mq3jUb_0LPn.Is:oGLCA2qhH6Q3'H,$DAp[;Z*:HZ='P\\8ee*UXK%6[...\n",
            "----------\n",
            "Source 4: maintenance_job_card.pdf (Similarity: 0.0760)\n",
            "Content: %U^k*#?YPCMk#]hfI'/AF2X:^QL#kdiDHgJaQPpYNUa]W!6Rj;@QKHE#kr\"Ke'r2a<db`7*#c%A^j<kOG%_'2\\s2KVnXYW0_5@EE7Y_+/aKWMTe0tP$b-Ke)bXj>q%B3/aNIjU#-A+&HQ0;ROc#^WY^LHCc>92h^*')WTs35JD%0CQS/NK9b,$LU+V4oG(tMD.K_L]#W...\n",
            "----------\n",
            "Source 5: engineering_drawing_specifications.pdf (Similarity: 0.0681)\n",
            "Content: Gau0BCN%rc'`FV1EI\\%C@oSnqQQQ1e=&TWfS&PeiTe6[TU18(e\\.mc76[nqcD9GZn8&g3V!\\BNZmlA&Tn;%Jgh/)s-V*I8\\0CKdGNF2eLRP'3L0FBGH?*O:K<Z?M.VSUbpD>WD3\"\\iK^83VWiU=`kfNaga(L<iQ6VO'T-`b9BUSr?-_j2j05GJ2o,F*]ndN8K[$]jUbX...\n",
            "----------\n",
            "---------------\n",
            "\n",
            "Your question: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd90c661"
      },
      "source": [
        "## Add provided helper functions\n",
        "\n",
        "### Subtask:\n",
        "Include the `query_ollama_api` and `chat_with_ollama` functions in the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "021f03cc"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to include the `query_ollama_api` and `chat_with_ollama` functions in the notebook. The `query_ollama_api` function is already present in the notebook from a previous step. The `chat_with_ollama` function was mentioned in the original code but not included in the previous steps. I will add the definition for `chat_with_ollama` to the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34c2a300"
      },
      "source": [
        "async def chat_with_ollama(api_url: str, model_name: str, messages: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"Chats with the Ollama API using the chat endpoint.\"\"\"\n",
        "    url = f\"{api_url}/api/chat\"\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"messages\": messages,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        result = response.json()\n",
        "        return result.get(\"message\", {}).get(\"content\", \"\").strip()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error chatting with Ollama API: {e}\")\n",
        "        return f\"Error chatting with Ollama API: {e}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbb82998"
      },
      "source": [
        "## Address connection issues\n",
        "\n",
        "### Subtask:\n",
        "Note that the provided code still uses `localhost:11434`, which is unlikely to work directly in Colab. Remind the user that they will need to adjust the URL to point to their accessible Ollama instance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eabbebf"
      },
      "source": [
        "**Reasoning**:\n",
        "Add a markdown cell to inform the user about the Ollama API URL configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "624c57ef",
        "outputId": "d4871560-98f3-479b-c6a3-b3c6c01e565b"
      },
      "source": [
        "### Ollama API URL Configuration\n",
        "\n",
        "Please note that the current RAG system code is configured to connect to the Ollama API at `http://localhost:11434`.\n",
        "\n",
        "This address is typically used for a local Ollama installation running on the same machine. When running this notebook in an environment like Google Colab, or if your Ollama instance is running elsewhere, this address will likely not work directly.\n",
        "\n",
        "**Action Required:**\n",
        "\n",
        "If you are running Ollama on a different machine or need to access it externally, you must replace `http://localhost:11434` in the code (specifically in the `CustomOllamaLLM` initialization within the `RAGSystem` class) with the actual accessible URL and port of your running Ollama instance."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2375610762.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2375610762.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Please note that the current RAG system code is configured to connect to the Ollama API at `http://localhost:11434`.\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f6c9d6e"
      },
      "source": [
        "**Reasoning**:\n",
        "Add a markdown cell with the specified text to inform the user about the Ollama API URL configuration. The previous attempt failed because the markdown content was placed in a Python code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "UdstyYQdp5wZ",
        "outputId": "51bab7af-32c6-44ae-881f-4f3f2e8ee9d5"
      },
      "source": [
        "# This is a markdown cell. Please convert this cell to markdown.\n",
        "\"\"\"\n",
        "### Ollama API URL Configuration\n",
        "\n",
        "Please note that the current RAG system code is configured to connect to the Ollama API at `http://localhost:11434`.\n",
        "\n",
        "This address is typically used for a local Ollama installation running on the same machine. When running this notebook in an environment like Google Colab, or if your Ollama instance is running elsewhere, this address will likely not work directly.\n",
        "\n",
        "**Action Required:**\n",
        "\n",
        "If you are running Ollama on a different machine or need to access it externally, you must replace `http://localhost:11434` in the code (specifically in the `CustomOllamaLLM` initialization within the `RAGSystem` class) with the actual accessible URL and port of your running Ollama instance.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n### Ollama API URL Configuration\\n\\nPlease note that the current RAG system code is configured to connect to the Ollama API at `http://localhost:11434`.\\n\\nThis address is typically used for a local Ollama installation running on the same machine. When running this notebook in an environment like Google Colab, or if your Ollama instance is running elsewhere, this address will likely not work directly.\\n\\n**Action Required:**\\n\\nIf you are running Ollama on a different machine or need to access it externally, you must replace `http://localhost:11434` in the code (specifically in the `CustomOllamaLLM` initialization within the `RAGSystem` class) with the actual accessible URL and port of your running Ollama instance.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fb4263e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A custom Langchain LLM class (`CustomOllamaLLM`) was successfully created to interface with the Ollama API, specifically targeting the `/api/generate` endpoint.\n",
        "*   The `CustomOllamaLLM` class was updated to implement both the synchronous `_call` and the required `_generate` methods from the `BaseLLM` abstract class, ensuring compatibility with Langchain's framework.\n",
        "*   The `RAGSystem` class was modified to initialize and use the `CustomOllamaLLM` with the `llama3-chatqa` model.\n",
        "*   Helper functions `query_ollama_api` (already present) and `chat_with_ollama` were included in the code.\n",
        "*   The `main` function's compatibility with the updated `RAGSystem` was confirmed through initialization and simulated usage, although actual query execution depends on a running Ollama instance.\n",
        "*   A note was added to inform the user about the default `localhost:11434` Ollama URL and the need to change it for different environments (like Colab).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The system is now configured to use the specified `llama3-chatqa` model via Ollama. The next step is to ensure an Ollama instance with this model is running and accessible at the configured URL for the RAG system to function correctly.\n",
        "*   Consider enhancing the `CustomOllamaLLM` or `RAGSystem` to gracefully handle Ollama API connection errors or model unavailability, providing clearer feedback to the user.\n"
      ]
    }
  ]
}